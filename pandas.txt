								# THIS IS FOR THE PANDASI


1 To give index to all row
  cmd	bric.index = ["a","b","c","d"] 

2 To set the index for the file [if neccessary]
  cmd  	bric=pd.read_csv("/path",  index_col=0)      //index_col=0 is used to set the reference for the the dataframe

3 To create dataframe from dictinory
  cmd bri=pd.DataFrame(dictionary)

4 If you select like this is mainly for columns
  cmd	bricks['col_namie']  	 //You will see the data is in series which is not good
  cmd   bricks[['col_name']] 	 // To get data as dataframe

5 For row select
  cmd 	brick[1:4]		// use slicing or index

---------------------------------------------------------------
 Series is good if you want to compare something like each value to other rather than dataframe
---------------------------------------------------------------

  # Simply this above approch has limitation as its like NdArray


6 NEW INTRODUCTION to LOC and ILOC

  cmd bricks.loc[["row index name"]]	// we will get dataframe rather than series & all the columns

  cmd bricks.loc[["row_index_name], ["col1_name","col2_name" ] ]  	// to get some specific column data


	NOTE -- ILOC and LOc is same the difference is that how you access data 
        	ILOC - we referred using index 				 brick[:,[0,1]]

        	LoC  - We referred using index_name of column		 brick[['row_name'],['col_name']]
 
7 To get the lenngth of string and new column in dataframe
  cmd     bric[name_new_column] = bric["col_you_waana_apply"].apply(len)      		//this will create new col with the len of string of col

8 To get rows printed using for loop

  cmd  	  for lab,row in bric.iterrows():
		print(lab +"="+ row['col_name']           


9 To divide and iterate large data
	for chunk in pd.read_csv("file.name" , chunksize=100)      //chunksize will be tbe number of partition
	

10 For excel file to load use
	pd.ExcelFile(name of file)




11 df2 = xl.parse(1, parse_cols=[0], skiprows=[0], names=['Country'])	     // to parse spreadsheet 1 with col first, 1st row is skipped ,on country

	To aceess particular sheet
->	dictinoary[sheetname  or index]

-------------------------------------------------
	SAS file [Great for all analyst]
	sas - statistical Analysis System [business analystics ]
	stats - statistics + data	[academic social science research]
-------------------------------------------------

   ####-  from sas7bdat import SAS7BDAT

		make file object by opening file
	NOTE -> 	-  file.to_data_frame()	   //to convert to dataframe


   -	For stata files
	data = pd.read_stata('filename')		


----------------------
 HDF5 -- Hierachical Data Format version 5
	 For huge dataset

	import h5py
----------------------

	data=h5py.File("filename",'r')

	for num in data.keys()		// To access the data keys as dictionary


------------
MATLAB FILeS
-------------

->	import scipy.io
-> 	scipy.io.loadmat('filename')		//.savemat() to save mat files

	Dictinary keys of python is Matlab variable names
	Dictinary values of python are assigned to variable
-> 	mat.keys()				// To print all the keys




15  DataBases 
        from sqlalchemy import create_engine
        engine = create_engine("db:///name.sqlite")
        table_name = engine.table_names()
       
        --> import packages, createEngine , connect to db, execute the querry
       
                                                                                                                                                      
        con=engine.connect()
        rs=con.execute("statemt")
-->     df=pd.DataFrame(rs.fetchall())                          // fetchmany(size=3)            only some specific data
        
        [df.columns = rs.keys()]        //if column  name is not good
       
        con.close()
       
        # Set the DataFrame's column names
        df.columns=rs.keys()
       
        with engine.connect() as con:
                rs = con.execute("querry")
       
       
       
    ALTERNATIVE --   pd.read_sql_query("querry ",  engineName)
                                                inner JOIN on tabl1.orderNo=table2.order2
                                                                                                                                    75,0-1        83%





	pd.info()					//to get info about all the coloumns in dataset
	df.columnName.value_counts(dropna=False)	//value_counts is a method to count number of same data in a column
	df.describe()					//only works with column with numeric data

	df.coloumnName.plot('hist')		//to plot the graph as matplotlib should be defined as earlier and hist is type of graph


	df.Boxplot(column='population', by='continent')			//outliers, min,max and 25,50,75 percentile



-----------
Pivoting = unmelting   turn unique values into seprate columns
-----------


weather = weather.pivot_table(values='value', index='date' , columns='element' , aggfunc=np.mean)
	// index is colimn you want to fix the pivot and columns contains those you want to pivot 


object.index 		//to get the index
There's a very simple method you can use to get back the original DataFrame from the pivoted DataFrame: .reset_index()




PROBLEM -- When a column contain multiple values
	1 Provide column with id_vars=[names]  for fixing some column value    // in pd.melt()
	2 df['newCol'] = df.fromCOL.str[0]  //likewise to get 1st data

	3 To split a column 
		ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')
	4 To access particular index of splitted column
		ebola_melt['type'] = ebola_melt.str_split.str.get(0)
	


TO CONCATINATE DataFrames
	pd.concat([df1,df2])	// index label can be same here so use

	pd.concat([df1,df2] , ignore_index=True)		// You will have index value different after concat too

	Note -- By defalult axis is 0 hence row wise concat

	-> For column wise concat use axis=1
		pd.concat([df1,df2] , ignore_index=True , axis=1)






-----------------------------------
GLOBBING -- pattern matching for files names
	*.csv	for any files with .csv extension
	_?.csv	any single character_file


	1 	import glob
		csv_files = glob.glob('*.csv')


	2 	USING LOOP
		list_data=[]
		for filename in csv_files:
			data=pd.read_csv(filename)		// to read csv files
			list_data.append(data)			// to append the readed files in list

		pd.concat(list_data)				// we are using this list to concat the dataframes



--------------------------------
MERGING Of dataframes
	
	pd.merge(left=df1 , right=df2 ,on=None , left_on='state',right_on='name')

------------------------------------

DATA-TYPES 	print(df.dtypes)

	1 To convert object column to string

		df['col_name] = df[col_name].astype(str)

	2 For sex column
		df['sex] = df['sex'].astype('category')			//sex is a type of category datatype


	3 To numeric value and blank values

		df[col] = pd.to_numeric(df[col] , errors='coerce')	//errors='coerce' here turn string "-" to NULL




--------------------------
To give external names to column of dataframes

	colName=['a','b','c']
	df = pd.read_csv(filename ,  header=None , names=colName )	//names is used to give column name to dataframe

	df = pd.read_csv(filename ,  header=None , names=colName , na_values=' -1' )	//na_values is used to detect the -1 in df as null & replace 													as NAN



To get the column values as numpy arrays to plot on MATPLOTLIB

		close = df['colName'].values		//column value in numpy
		type(close)				// check its type	
		plt.plot(close)

	OR can plot   close=df['colName']
		      plt.plot(close)



-------------------------------------------------
	To get the quantiles in dataframe for a list
	df.quantile([0.5,095])				//give the range value inside the list



	df['col'].unique()				//gives all unique value available in a column of df

	------------------------------------------------------------------
	*	*	*

	To get a particular set of value in a new df
	new_ = df['colName'] == 'VALUE'		//new df with the filtered value
	newDF = df.loc[new_ , : ]		// this will create to search that filtered value in all data frame

	*	*	*	
	-----------------------------------------------------------------

To index and read data in ISO format

	df3 = pd.read_csv(filename, index_col='Date', parse_dates=True)


To convert string into pandas datatime 
	pd.to_datetime(['2015-12-22 21:00']) 		or give format='format inside'		//it convert string to datatime object


	REINDEXING sometime required to match data as required

		df.reindex(last dataframe or column )
		
		To fill NAN values
			df1.reindex(df , method='ffill')  //forwardfill nan value are filled with nearest proceding value
			df1.reindex(df , method='bfill')  //backwardfill opposite to forwardfill


	
	
RESAMPLING -- It is used to find statistics according to data , time or year or other stuff

	daily_mean = df.resample('D').mean 	// resample method is used here ['D'] here is used for the day resampling in date 
							'W' is for week
							'D' is for day
							'min' minute
							'H'  hour
							'A'   Year
							'Q'  quarter
							'M'   Month
							'B'  Bussiness day




Rolling means (or moving averages) are generally used to smooth out short-term fluctuations in time series data and highlight long-term trends.
To use the .rolling() method, you must always use method chaining, first calling .rolling() and then chaining an aggregation method after it. For example, with a Series hourly_data,
		 hourly_data.rolling(window=24).mean()

To use the .rolling() method, you must always use method chaining, first calling .rolling() and then chaining an aggregation method after it. For example, with a Series hourly_data, hourly_data.rolling(window=24).mean()





-----------------
	df['col'].str.upper()			//for change in upper case
		 .str.contains('data')		//to match substrig in the column


Set local time zone first
	central = df['Date'].dt.tz_localize('US/Central')

	central.dt.tz_convert('US/Eastern')			//to convert to Eastern timezone



Like for a df with time gap of 10 years if we extract values year wise we will have NAN value in those years hence [interpolate] is good to fill values linearly than forwardfill meathod.
	

	df.resample('A').first().interpolate('linear')





pandas with matplotlib different style
	df.plot(style='k.-')
			color 		k
			marker	 	.
			linetype	- [solid]


To retrive data in particular timestamp

	df.loc['2012-02-22' : '2015-06-23'  , 'colname']




-------------------------------------------------------------------
	To make all dataframe conversion to specific values

	df.floordiv(12) 						// converting to dozens unit of dataframe

	np.floor_divide(df,12)




To assign any index
	df.index = df['colName']			//that column will work as index for the dataframe
	df.index.name = 'new name'			// this is the new name of index of a df 
	del df['colName']				// this will delete that duplicate column


	df.set_index(['col1','col2'])			// to set the index of col in a df
	df.sort_index()					// to sort the index values 



	df.loc['colName', (2),:]		// 2 here is the value for index sorted and filtered value
	df.loc[(slice(None),2),:]	        //Use (slice(None), 2) to extract all rows in month 2
---------------

RESHAPING THE DATAFRAME
	df.pivot(index='col1' , columns='col2' , values	='col3')	//col1 is index , col2 will be the columns and col3 will be the value in df


   --	To divide same values to the columns
	
	df.unstack(level='gender') 				// all column will get the same value 

   --	To make a wide df into smaller 
	
	df.stack(level='gender')				// wider df into small

   -- 	To switch the index level
	df.swaplevel(0,1)					// first index switch to second and vice versa
		.sort_index						apply this to sort index

--------------------------
Groupby
	df.groupby('colName').agg(['sum','count'])

	df.groupby('colName').agg({'colname':'sum',  'col2':'count'])





